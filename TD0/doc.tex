\documentclass[a4paper,francais]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[french]{babel}

\usepackage{subfig}
\usepackage{graphicx}
\graphicspath{{fig/}}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{cancel}

\usepackage{hyperref}

\usepackage{cprotect} %verbatim in footnote

\usepackage[english,linesnumbered,ruled,vlined]{algorithm2e}

\newcommand{\cad}{c.-à-d.}
\newcommand{\Z}{{\ensuremath\mathbb{Z}}}
\newcommand{\N}{{\ensuremath\mathbb{N}}}
\newcommand{\R}{{\ensuremath\mathbb{R}}}
\newtheorem{Theorem}{Theorem}

%-------- enable or disable correction -----------------------------
\theoremstyle{definition}
\newtheorem{exercice}{Exercice}[section]
\newtheorem*{solution}{Solution}

\usepackage{comment}
\excludecomment{solution}% commenter/décommenter pour afficher/effacer l'impression des solutions


\title{Optimisation continue}
\author{Tristan Roussillon}
\date{fév. 2026}

\begin{document}

\maketitle

Dans ce TD, l'objectif est de se familiariser
avec quelques outils et résultats fondamentaux
en optimisation.  

\section{Prérequis : valeurs propres}
\label{sec:calcul}

\begin{exercice}
  Soit $M$ une matrice carrée. Un vecteur non nul $\vec{x}$ qui, associé
  à un scalaire $\lambda$, vérifie $M\vec{x} = \lambda \vec{x}$ est appelé
  vecteur propre. Le scalaire associé est appelé valeur propre.

  \begin{enumerate}
  \item Donnez les valeurs propres d'une matrice $kI$, $I$ étant la matrice identité. 
  \item Sachant qu'un système d'équations linéaires homogène $A\vec{x} = \vec{0}$ admet des
    solutions non triviales si et seulement si $\det{A} = 0$, montrez que les valeurs propres
    de $M$ sont les scalaires $\lambda$ tels que $\det{(M - \lambda I)} = 0$. 
  \item On suppose maintenant que $M$ est une matrice $2 \times 2$. Montrez que
    les valeurs propres de $M$ sont les solutions de l'équation
    \[ \lambda^2 - \lambda \ \text{trace} \ (M) + \det{(M)} = 0.\]
  \item On suppose maintenant que $M$ est symétrique.
    Montrez que ses valeurs propres sont réelles. 
  \end{enumerate}
  
\end{exercice}

\begin{solution}
  1. Pour tout $\vec{x}$, on a $kI\vec{x} = k \vec{x}$. Par définition $k = \lambda$
  est la valeur propre de $kI$ associée à tout $\vec{x}$. 
  
  2. A partir de  $M\vec{x} = \lambda \vec{x}$, en retranchant $\lambda \vec{x}$ et en mettant
  $\vec{x}$ en facteur on obtient $(M - \lambda I) \vec{x} = 0$. C'est un système
  d'équations linéaires homogène. Une solution triviale est $\vec{x} = \vec{0}$.
  Il y a d'autres solutions ssi $\det{(M - \lambda I)} = 0$. 

  3. En posant $M = \begin{pmatrix} a & b \\ c & d \end{pmatrix}$, on a :
  \begin{align*}
    \det{(M - \lambda I)} &=
    (a-\lambda)(d - \lambda) - cb \\
    &= ad - a\lambda - d\lambda + \lambda^2 - cb \\
    &= \lambda^2 - \lambda(a+d) + (ad -cb).
  \end{align*}

  4. Le discriminant du polynôme de second degré est $(a+d)^2 - 4(ad-cb)$.
  Mais en posant $c=b$ (symétrie), on déduit que
  \begin{align*}
    (a+d)^2 - 4(ad-b^2) &=
    a^2 + d^2 + 2ad - 4ad + 4b^2 \\
    &= a^2 + d^2 - 2ad + 4b^2 \\
    &= (a-d)^2 + 4b^2 \geq 0.
  \end{align*}
  Comme le discriminant est positif, le polynôme possède des racines réelles.% :
  %% \[ \lambda = \frac{(a+d) \pm \sqrt{\big( (a-d)^2 + 4b^2 \big)}}{2}. \]
  %% (Si $b=0$ et $a=d$, on retrouve le résultat 1.)
\end{solution}

\section{Conditions d'optimalité locale et globale}
\label{sec:opt-loc}

\begin{exercice}
  On considère successivement les fonctions $x$, $x^2$, $-x^2$ et $x^3$.
  Pour chacune d'elles, indiquez si les deux conditions d'optimalité
  locale sont satisfaites ou non en $0$.
\end{exercice}

\begin{solution}
  Les deux conditions sont :
  \begin{enumerate}
  \item le gradient est nul 
  \item le hessien est une matrice définie positive (valeurs propres $>$ 0)
  \end{enumerate}
  au point considéré, ici $x=0$. 
  
  \[
  \begin{array}{c|c|c|c|c}
    f & f' & f'' & f'(0) = 0? & f''(0) > 0? \\ \hline
    x & 1  & 0 & \text{non} & \text{non}  \\
    x^2 & 2x & 2 & \text{oui} & \text{oui}\\
    -x^2 & -2x & -2 & \text{oui} & \text{non} \\
    x^3 & 3x^2 & 6x & \text{oui} & \text{non}
  \end{array}
  \]
  La première condition n'est pas vérifiée dans le premier cas. Dans les trois suivants si.
  La seconde condition n'est vérifiée que dans le deuxième cas, où $0$ est donc minimum local
  (et même un minimum global).
  %Dans les cas 3 et 4, $0$ est respectivement est maximum et un point d'inflexion. 
\end{solution}

\begin{exercice}
  Même exercice que précédemment en considérant 
  les fonctions à deux variables  $x_1^2 + x_2^2$, $-x_1^2 -x_2^2$,
  $x_1^2 -x_2^2$, $x_1^2 + x_2$, $x_1^3 - 3x_1x_2^2$
  en $(x_1 = 0, x_2 = 0)$.  
\end{exercice}

\begin{solution}
  \[
  \begin{array}{c|c|c|c|c}
    f & {\nabla f} & H_f & {\nabla f}(\vec{0}) = \vec{0}? & \text{type $H_f$} \\ \hline
    x_1^2 + x_2^2 & (2x_1,2x_2)  & \begin{pmatrix} 2 & 0 \\ 0 & 2 \end{pmatrix} & \text{oui} & \text{def.pos.}  \\
    -x_1^2 - x_2^2 & (-2x_1,-2x_2) & \begin{pmatrix} -2 & 0 \\ 0 & -2 \end{pmatrix} & \text{oui} & \text{def.neg.}\\
    x_1^2 - x_2^2 & (2x_1,-2x_2) & \begin{pmatrix} 2 & 0 \\ 0 & -2 \end{pmatrix} & \text{oui} & \text{indef.} \\
    x_1^3 - 3x_1x_2^2 & (3x_1^2 - 3x_2^2, 6x_1x_2) & \begin{pmatrix} 6x_1 & -6x_2 \\ -6x_2 & -6x_1 \end{pmatrix} & \text{oui} & \text{nul}
  \end{array}
  \]

  La condition de stationnarité est vérifiée dans tous les cas. 
  La condition sur le hessien n'est vérifiée qu'en première ligne, où $\vec{0}$ est donc un minimum
  local (et même global).

  En ligne 3, on trouve facilement les valeurs propres, comme les racines
  de $\lambda^2 - \lambda \ \text{trace} \ (H_f) + \det{(H_f)}$ : $2,-2$ (point selle). 
\end{solution}

\begin{exercice}
  Soit la fonction suivante: $f(x_1,x_2) = 3x_1^2 + 2x_2 + 1$.
  \begin{enumerate}
  \item Calculez le gradient et le hessien de $f$. 
  \item Est-ce que $f$ est convexe ?
  \item Expliquez pourquoi trouver $x_1,x_2$ qui minimisent $f$
    est un problème non borné.
  \end{enumerate}
\end{exercice}

\begin{solution}
  $\nabla f(x_1,x_2) = (6x_1 + 2, 2)$ et $H_f(x_1,x_2) = \begin{pmatrix} 6 & 0 \\ 0 & 0 \end{pmatrix}$.
  Ses valeurs propres sont $6$ et $0$. C'est une matrice semi-définie positive. D'après le théorème de
  convexité, $f$ est convexe. Le minimum global, s'il existe, annule le gradient. Mais le gradient ne
  peut être annulé (la deuxième composante est la constante $2$). Le problème de minimisation est
  en effet non borné car $f(x_1,x_2) \rightarrow \infty$ quand $x_2 \rightarrow \infty$.   
\end{solution}

\begin{exercice}
  Soit la fonction suivante: $f(x) = x^4 - x^2 + 1$.
  \begin{enumerate}
  \item Calculez la dérivée première et seconde de $f$. 
  \item Est-ce que $f$ est convexe ?
  \item Montrer que $0$ est un minimum local.
  \item Comparez $f(0)$ et $f(1/\sqrt{2})$. Qu'en déduisez-vous ?
  \end{enumerate}
\end{exercice}

\begin{solution}
  1. 2. On a $f(x) = x^4 - x^2 + 1$, $f'(x) = 4x^3 - 2x$, $f''(x) = 12x - 2$
  ($f$ n'est pas convexe, car on n'a pas $f''(x) > 0$ pour tout $x$). 

  3. En $0$, on a $f(0) = 1$, $f'(0) = 0$ et $f''(0) = -2$, les deux conditions d'optimalité locale sont vérifiées.

  4. Comme $f(1/\sqrt{2}) = 1/4 - 1/2 + 1 = 3/4$, $0$ n'est pas un minimum global.  
\end{solution}

\section{Optimisation sous contrainte}
\label{sec:kkt}

\begin{exercice}
  \label{ex:ex}
  Considérons les quatre propositions suivantes :
  \begin{itemize}
  \item[(a)] Le problème est infaisable,
  \item[(b)] Le problème est non borné,
  \end{itemize}
  et, en notant $X$ l'ensemble des
  solutions candidates vérifiant les contraintes,
  \begin{itemize}
  \item[(c)] La solution optimale se trouve sur un sommet du bord de $X$,
  \item[(d)] Les solutions optimales se trouvent sur une arête du bord de $X$.
  \end{itemize}
  
  Considérons également les 4 problèmes suivants : trouver $x_1,x_2 \in \R$ qui minimisent 
  \begin{itemize}
    \item[(P1)] $-x_1 - x_2 - 1$ et vérifient $x_1,x_2 \geq 0$,   %(b) non borné
    \item[(P2)] $x_1$ et vérifient $x_1,x_2 \geq 0$, $x_1 + x_2 + 1 \leq 0$, %(a) infaisable
    \item[(P3)] $x_1$ et vérifient $x_1,x_2 \geq 0$, $x_1 + x_2 - 1 \leq 0$, %arete
    \item[(P4)] $-x_1+x_2$ et vérifient $x_1,x_2 \geq 0$, $x_1 + x_2 - 1 \leq 0$. %sommet
  \end{itemize}
  
  Donnez la correspondance entre les 4 propositions et les 4 problèmes
  à l'aide de représentations graphiques.
\end{exercice}

\begin{solution}
  (P1) (b), (P2) (a), (P3) (d), (P4) (c). 
\end{solution}

\begin{exercice}
  On considère maintenant plus généralement un problème d'optimisation linéaire. 
  On note $X$ l'ensemble des solutions candidates vérifiant les contraintes et
  on suppose que la fonction objectif $f$ n'est pas constante. Montrez par
  contradiction et en utilisant les conditions KKT qu'une solution optimale 
  ne peut appartenir à l'intérieur de $X$.
\end{exercice}

\begin{solution}
  Si $x \in X \setminus \partial X$, par définition aucune contrainte n'est saturée,
  ce qui implique que tous les coefficients $\lambda_i$ sont nuls. La condition
  se simplifie en ${\nabla f}(x) = 0$, ce qui est faux pour $f$ linéaire et non constante.
\end{solution}

\begin{exercice}
  Soit le problème suivant : trouver $x_1, x_2 \in \R$ qui minimisent $x_1^2 + x_2^2$
  sous la contrainte $ax_1 + bx_2 + c \leq 0$. 

  \begin{enumerate}
  \item Donnez une interprétation géométrique du problème. Expliquez pourquoi il y a deux
    cas différents à traiter. 
  \item En utilisant les conditions KKT, donnez la solution du problème dans les deux cas.  
  \end{enumerate}
\end{exercice}

\begin{solution}
  Les lignes de niveau de la fonction $f(x_1,x_2) = x_1^2 + x_2^2$, c'est-à-dire
  les points $(x_1,x_2)$ vérifiant $f(x_1,x_2) = r$ sont des cercles centrés en $(0,0)$. 
  La contrainte est une inégalité affine, ce qui correspond à un demi-plan.

  Remarquons que sans contrainte, le minimum est évidemment le centre, en $(0,0)$.
  C'est également le cas, si $(0,0)$ respecte la contrainte. Il peut en être autrement
  que si $(0,0)$ ne respecte pas la contrainte. 
  
  Comme il n'y a qu'une contrainte, les conditions KKT impliquent deux cas :
  \begin{itemize}
  \item soit aucune contrainte n'est saturée. Dans ce cas, le minimum annule
    le gradient $\nabla f(x_1,x_2) = (2x_1,2x_2)$: c'est $(0,0)$ (cohérent avec notre remarque). 
  \item soit une contrainte est saturée. Dans ce cas, le minimum vérifie
    \[ ax_1+bx_2+c = 0, \quad \text{et} \quad (2x_1,2x_2) = \lambda(a,b). \]
    C'est un système de trois équations à trois inconnues ($x_1, x_2,\lambda$).
    On obtient
    \[ x_1 = \lambda a, \quad x_2 = \lambda b, \quad \lambda = \frac{-2c}{a^2+b^2}. \]
  \end{itemize}
\end{solution}

\section{Bonus : dérivation vectorielle/matricielle}

\begin{exercice}
  Soient un vecteur $\vec{a} = (a_1,\dots,a_d) \in \R^d$ et
  un scalaire $s \in \R$.
  Calculez le gradient des fonctions:
  \begin{enumerate}
  \item $f(\vec{x}) = \vec{x}\cdot\vec{a}$,  
  \item $f(\vec{x}) = (\vec{x}\cdot\vec{a})^2$,
  \item $f(\vec{x}) = (s - \vec{x}\cdot\vec{a})^2$. 
  \end{enumerate}
\end{exercice}

\begin{solution}
  La fonction $f$ est une fonction à $d$ variables : $f(\vec{x}) = f(x_1,\dots,x_d)$.
  Le gradient est par définition le vecteur des dérivées partielles :
  ${\nabla f}(x_1,\dots,x_d) = (\frac{\partial f}{\partial x_1}, \dots, \frac{\partial f}{\partial x_d})$.

  1. Si on développe le produit scalaire $\vec{x}\cdot\vec{a} = \sum_{i=1}^dx_ia_i = x_1a_1 + \dots + x_da_d$,
  on trouve facilement que
  \[ \frac{\partial f}{\partial x_k} = \frac{\partial}{\partial x_1}(\sum_{i=1}^dx_ia_i) = a_k.\]
  (la dérivée du produit $x_ia_i$ par rapport à $x_k$ est $0$ si $i\neqk$ et $a_k$ si $i=k$).
  On conclut donc que ${\nabla f}(\vec{x}) = \vec{a}$. 

  2. Encore une fois, on développe le produit scalaire
  \begin{align*}
    (\vec{x}\cdot\vec{a})^2 &= \big( \sum_{i=1}^dx_ia_i \big)\big( \sum_{i=1}^dx_ia_i \big) \\
    &= \sum_{i=1}^dx_ia_i \big( \sum_{j=1}^dx_ja_j \big).  \]
  Ensuite, on dérive
  \[ \frac{\partial f}{\partial x_k} = \frac{\partial}{\partial x_1}\bigg( \sum_{i=1}^dx_ia_i \big( \sum_{j=1}^dx_ja_j \big) \bigg)
  = 2a_k (\sum_{i=1}^d x_ia_i).\]
  (la dérivée du produit $x_ia_ix_ja_j$ par rapport à $x_k$ est $0$ si $k\neq i, k\neq j$,
  $a_kx_ja_j$ si $k=i, k\neqj$ et $a_ix_ia_k$ si $k\neq i, k=j$).
  On conclut donc que ${\nabla f}(\vec{x}) = 2(\vec{x}\cdot\vec{a})\vec{a}$. 

  3. On développe le carré et on utilise les deux résultats précédents pour obtenir
  ${\nabla f}(\vec{x}) = -2s\vec{a} + 2(\vec{x}\cdot\vec{a})\vec{a}$. 
\end{solution}

\newcommand\mat[1]{\mathbf{#1}}

\begin{exercice}
  Soient les matrices
  \begin{itemize}
    \item $\mat{x}$ de taille $n \times d$, $\mat{y}$ de taille $n \times 1$ (ensemble d'apprentissage),
    \item $\mat{W}_1, \mat{W}_2$ de taille respectivement $k \times d$ et $1 \times k$ (poids du réseau),
  \end{itemize}
  et une fonction d'activation $\sigma$.
  
  Soit la fonction de perte :
  \[ L(\mat{W}_1, \mat{W}_2) := \frac{1}{2n} \sum_{i+1}^n (\mat{y}_i - \hat{\mat{y}}_i)^2, \]
  où $\hat{\mat{y}} = \mat{W}_2 \sigma ( \mat{W}_1 \mat{x} )$. 

  Calulculez le gradient de la fonction de perte. 
\end{exercice}

%% \begin{solution}
%%   Définissons les variables intermédiaires :
%%   \begin{itemize}
%%   \item $\mat{Z} := \mat{W}_1 \mat{x}$ de taille $k \times n$.
%%   \item $\mat{A} := \sigma(Z)$ de taille $k \times n$ ($\sigma$ est appliquée élément par élément). 
%%   \end{itemize}
%% \end{solution}


%% other exercises
%% \appendix

%% \begin{exercice}
%%   Soient $g_1(x_1,x_2) = 10x_1 + 12x_2 - 59$, $g_2(x_1,x_2) = -x_1$, $g_3(x_1,x_2) = -x_2$.
%%   On cherche $x_1,x_2 \in \mathbb{R}$ qui minimisent la fonction objectif $f(x_1,x_2) = -10x_1 -11x_2$
%%   et tels que $\forall i \in \{1,\dots,3\}$, $g_i(x_1,x_2) \leq 0$. 
%%     \begin{enumerate}
%%     \item Indiquez clairement le nombre d'inconnus, la fonction objectif et les contraintes.
%%     \item Comment s'appelle ce type de problème ? Citez deux algorithmes de résolution. 
%%     \item Donnez une représentation graphique du problème. 
%%     \item Rappelez les conditions de Kuhn et Tucker et indiquez pourquoi elles peuvent
%%       s'appliquer à ce problème.
%%     \item En utilisant les conditions de Kuhn et Tucker, ainsi que les gradients de $f$, $g_1$, $g_3$,
%%       montrez que la solution du problème est le point $(x_1^\star,x_2^\star)$ où $g_1$ et $g_3$ sont saturées.   
%%     \end{enumerate}
%% \end{exercice}

%% Soit la fonction suivante :  $f(x_1,x_2) = 2x_1x_2 + 2x_2 - x_1^2 -2x_2^2$. 
%%     \begin{enumerate}
%%     \item Calculez le gradient et le hessien de $f$. 
%%     \item Qu'en est-il de la convexité de $f$ ? (Note: les valeurs propres d'une matrice $M$ $2\times2$
%%       sont les racines du polynômes $\lambda^2 - \text{trace}(M) \lambda + \text{det}(M)$). 
%%     \item On applique la méthode de la descente de gradient en partant du point de coordonnées $(1,1)$.
%%       Quel point obtient-on en se déplaçant d'un vecteur correspondant à l'opposé du gradient de $f$ en $(1,1)$ ?
%%     \end{enumerate}


\end{document}


